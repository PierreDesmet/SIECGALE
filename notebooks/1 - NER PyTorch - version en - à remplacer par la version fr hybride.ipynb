{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed671d04",
   "metadata": {
    "cell_style": "center"
   },
   "source": [
    "# 1 - NER PyTorch\n",
    "\n",
    "\n",
    "**Sources** :\n",
    "- Source de données Kaggle : https://www.kaggle.com/datasets/abhinavwalia95/entity-annotated-corpus\n",
    "- Tuto Abishek : https://www.youtube.com/watch?v=MqQ7rqRllIc\n",
    "\n",
    "**TODO**\n",
    "- [x] changer le padding par une autre valeur -> pas besoin en définitive.\n",
    "- [x] set num workers (minitrain, valid et test)\n",
    "- [x] peut-on monter la taille des batchs ?\n",
    "- [x] stratifier les splits\n",
    "- [x] enregistrer la running loss\n",
    "- [ ] est-on sûr que les special tokens ne contribuent pas à la loss ?\n",
    "- [ ] vérifier que le code <a href=\"https://www.kaggle.com/code/abhishek/entity-extraction-model-using-bert-pytorch\">ici</a> est le bon\n",
    "- [ ] gradual unfreezing\n",
    "- [ ] early stopping avec lightning\n",
    "- [ ] POS des mots autour\n",
    "- [ ] wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3368041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T18:42:54.168000Z",
     "start_time": "2022-07-26T18:42:52.360222Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0726 18:42:53.989986 140089878529856 modeling.py:230] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "os.chdir('..')\n",
    "\n",
    "from ner_pytorch.config.params import PARAMS\n",
    "from ner_pytorch.dataset import EntityDataset\n",
    "from ner_pytorch.engine import eval_fn, train_fn\n",
    "from ner_pytorch.model import EntityModel\n",
    "from ner_pytorch.preprocessing import process_data\n",
    "from ner_pytorch.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff67c25",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89d2905",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T09:09:21.136649Z",
     "start_time": "2022-07-11T09:09:20.348921Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>through</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>London</td>\n",
       "      <td>B-geo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word    Tag\n",
       "0  Sentence: 1      Thousands      O\n",
       "1  Sentence: 1             of      O\n",
       "2  Sentence: 1  demonstrators      O\n",
       "3  Sentence: 1           have      O\n",
       "4  Sentence: 1        marched      O\n",
       "5  Sentence: 1        through      O\n",
       "6  Sentence: 1         London  B-geo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(PARAMS.PATHS_EN.TRAIN, encoding='latin-1').drop('POS', axis=1)\n",
    "data[\"Sentence #\"] = data[\"Sentence #\"].fillna(method='ffill')\n",
    "data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e373f1c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "En réalité, nous ne nous intéressons qu'à la prédiction des ORG, pour lesquelles 3 labels sont possibles :\n",
    "- B-org\n",
    "- I-org\n",
    "- O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99c77a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T09:09:31.420625Z",
     "start_time": "2022-07-11T09:09:31.079298Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "O        0.964784\n",
       "B-org    0.019210\n",
       "I-org    0.016006\n",
       "Name: Tag, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tag'] = data.Tag.mask(~data.Tag.isin(['B-org', 'I-org', 'O']), 'O')\n",
    "data.Tag.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb99f73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T09:09:45.617701Z",
     "start_time": "2022-07-11T09:09:45.516647Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag 3 categories : ['O' 'B-org' 'I-org']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1048575, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>The</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>International</td>\n",
       "      <td>B-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>Atomic</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>Energy</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>Agency</td>\n",
       "      <td>I-org</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>is</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Sentence: 8</td>\n",
       "      <td>to</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentence #           Word    Tag\n",
       "153  Sentence: 8            The      O\n",
       "154  Sentence: 8  International  B-org\n",
       "155  Sentence: 8         Atomic  I-org\n",
       "156  Sentence: 8         Energy  I-org\n",
       "157  Sentence: 8         Agency  I-org\n",
       "158  Sentence: 8             is      O\n",
       "159  Sentence: 8             to      O"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tag = data.Tag.nunique()\n",
    "print(f'Tag {num_tag} categories :', data.Tag.unique(), end='\\n\\n')\n",
    "\n",
    "data.shape\n",
    "data[153:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "596ddef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:26:52.346399Z",
     "start_time": "2022-06-26T08:26:48.782232Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/outputs/ordinal_enc_NER.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences, tag, ordinal_enc_NER = process_data(data)\n",
    "joblib.dump(ordinal_enc_NER, 'data/outputs/ordinal_enc_NER.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a4e7a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:26:52.372052Z",
     "start_time": "2022-06-26T08:26:52.348727Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Beirut', ',', 'a', 'string', 'of', 'officials', 'voiced', 'their', 'anger', ',', 'while', 'at', 'the', 'United', 'Nations', 'summit', 'in', 'New', 'York', ',', 'Prime', 'Minister', 'Fouad', 'Siniora', 'said', 'the', 'Lebanese', 'people', 'are', 'resolute', 'in', 'preventing', 'such', 'attempts', 'from', 'destroying', 'their', 'spirit', '.']\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[array(['O', 'B-org', 'I-org'], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# Démo : \n",
    "i = 10\n",
    "print(sentences[i], end='\\n')\n",
    "print(tag[i], end='\\n')\n",
    "print(ordinal_enc_NER.categories_, end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590afff",
   "metadata": {},
   "source": [
    "## Split du jeu de données\n",
    "On split notre jeu de données de la façon suivante :\n",
    "- `test` = 20%\n",
    "- `train` = [`minitrain`, `valid`] = 80%\n",
    "- `minitrain` = 60%\n",
    "- `valid` = 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2941807d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:26:59.261911Z",
     "start_time": "2022-06-26T08:26:58.904193Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.964808\n",
       "1.0    0.019224\n",
       "2.0    0.015968\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0    0.965036\n",
       "1.0    0.018949\n",
       "2.0    0.016014\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(28777, 9591, 9591)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_test = int(PARAMS.SAMPLE_SIZES.TEST * len(sentences))\n",
    "len_valid = int(PARAMS.SAMPLE_SIZES.VALID * len(sentences))\n",
    "autre = ordinal_enc_NER.transform([['O']]).item()\n",
    "\n",
    "# En présence d'un jeu déséquilibré, il vaut mieux stratifier :\n",
    "strat_tag = [len([_ for _ in tag[i] if _ != autre]) > 0 for i in range(len(tag))]\n",
    "(\n",
    "    sentences_train, sentences_test,\n",
    "    tag_train, tag_test\n",
    ") = train_test_split(sentences, tag, random_state=PARAMS.SEED, \n",
    "                     test_size=len_test, stratify=strat_tag, shuffle=True)\n",
    "\n",
    "strat_tag_train = [len([_ for _ in tag_train[i] if _ != autre]) > 0 for i in range(len(tag_train))]\n",
    "(\n",
    "    sentences_minitrain, sentences_valid,\n",
    "    tag_minitrain, tag_valid\n",
    ") = train_test_split(sentences_train, tag_train, random_state=PARAMS.SEED, \n",
    "                     test_size=len_valid, stratify=strat_tag_train, shuffle=True)\n",
    "\n",
    "# Pour se rassurer sur la bonne représentativité de chaque classe dans les échantillons stratifiés : \n",
    "pd.Series(np.concatenate(tag_train)).value_counts(normalize=True)\n",
    "pd.Series(np.concatenate(tag_valid)).value_counts(normalize=True)\n",
    "\n",
    "len(sentences_minitrain), len(sentences_valid), len(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f7dba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:26:59.656983Z",
     "start_time": "2022-06-26T08:26:59.631566Z"
    }
   },
   "outputs": [],
   "source": [
    "minitrain_dataset = EntityDataset(\n",
    "    texts=sentences_minitrain, tags=tag_minitrain\n",
    ")\n",
    "minitrain_data_loader = DataLoader(\n",
    "    minitrain_dataset, batch_size=PARAMS.MODEL.TRAIN_BATCH_SIZE, num_workers=16 \n",
    ")\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "    texts=sentences_valid, tags=tag_valid\n",
    ")\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset, batch_size=PARAMS.MODEL.VALID_BATCH_SIZE, num_workers=16\n",
    ")\n",
    "\n",
    "test_dataset = EntityDataset(\n",
    "    texts=sentences_test, tags=tag_test\n",
    ")\n",
    "test_data_loader = DataLoader(\n",
    "    test_dataset, batch_size=PARAMS.MODEL.VALID_BATCH_SIZE, num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "621ebe1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:00.612207Z",
     "start_time": "2022-06-26T08:27:00.580994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Spanish', 'troops', 'will', 'join', 'the', 'European', 'Union', 'force', 'sent', 'to', 'protect', 'ships', 'against', 'hijackings', 'and', 'attacks', 'by', 'Somali', 'pirates', '.']\n",
      "ids: tensor([  101,  1996,  3009,  3629,  2097,  3693,  1996,  2647,  2586,  2486,\n",
      "         2741,  2000,  4047,  3719,  2114,  7632, 17364,  8613,  1998,  4491,\n",
      "         2011, 16831,  8350,  1012,   102,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "token_type_ids: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "target_tag: tensor([0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "i = 40\n",
    "print(test_dataset.texts[i])\n",
    "for key, value in test_dataset[i].items():\n",
    "    print(key + ':', value)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "489844e9",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Expériences pré-entraînement\n",
    "\n",
    "Avant de lancer le modèle, quelle _loss_ serait considérée comme statisfaisante ? Calculons à l'avance les loss que donneraient :\n",
    "\n",
    "- un modèle qui prédit aléatoirement \n",
    "- un modèle qui prédit la classe modale (\"O\")\n",
    "- un modèle qui prédit de façon satisfaisante, avec une accuracy donnée*\n",
    "- un modèle oracle (prédiction parfaite)\n",
    "\n",
    "*Remarque : pour deux modèles qui produisent la même accuracy fixée, l'un peut être meilleur que l'autre ! En effet, les probas prédites peuvent être plus ou moins proches de la réalité, même si elles peuvent résulter en des prédictions binaires identiques (et donc en la même accuracy).\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b90431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:07.845723Z",
     "start_time": "2022-06-26T08:27:07.382439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loss_ce = nn.CrossEntropyLoss(reduction='mean')\n",
    "y_true = torch.tensor(ordinal_enc_NER.transform(data[['Tag']]), dtype=torch.long).view(-1)\n",
    "\n",
    "y_true_ohe_parfait = torch.tensor(\n",
    "    pd.get_dummies(y_true).astype(float).replace(0, -1234).to_numpy(),\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "# -3 est un nombre arbitraire qui affecte à la bonne classe une proba de 5%.\n",
    "y_true_ohe = torch.tensor(\n",
    "    pd.get_dummies(y_true).astype(float).replace(0, -3).to_numpy(),\n",
    "    dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59db0bd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:08.713616Z",
     "start_time": "2022-06-26T08:27:08.404842Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss oracle : tensor(0.)\n",
      "Loss d'un modèle satisfaisant : tensor(0.1160)\n",
      "Loss du modèle qui prédit argcount : tensor(0.1768)\n",
      "Loss d'un modèle aléatoire : tensor(2.7005)\n"
     ]
    }
   ],
   "source": [
    "#  Modèle oracle :\n",
    "y_pred = y_true_ohe_parfait.detach()\n",
    "print('Loss oracle :', loss_ce(y_pred, y_true))\n",
    "\n",
    "# Modèle avec une accuracy de 98 %\n",
    "# Pour rappel, une accuracy en dessous de 96 % est mauvaise !\n",
    "# → data.Tag.value_counts(normalize=True)\n",
    "accuracy_souhaitée = 0.98\n",
    "nb_lignes_à_erroring = int((1 - accuracy_souhaitée) * y_true.shape[0])\n",
    "y_pred = y_true_ohe.detach()\n",
    "y_pred[0:nb_lignes_à_erroring, :] = shift_tensor(y_pred[0:nb_lignes_à_erroring, :])\n",
    "print(\"Loss d'un modèle satisfaisant :\", loss_ce(y_pred, y_true))\n",
    "\n",
    "# Modèle qui prédit la classe modale : \n",
    "classe_modale = data.Tag.value_counts().nlargest(1).index[0]\n",
    "y_pred = np.full((len(data), num_tag), -3)\n",
    "y_pred[:, int(ordinal_enc_NER.transform([[classe_modale]]).item())] = 1\n",
    "y_pred = torch.tensor(y_pred, dtype=torch.float)\n",
    "print(\"Loss du modèle qui prédit argcount :\", loss_ce(y_pred, y_true))\n",
    "\n",
    "# Modèle aléatoire :\n",
    "y_pred = torch.tensor(\n",
    "    pd.get_dummies(torch.randint(0, num_tag, (len(data),))).astype(float).replace(0, -3).to_numpy(),\n",
    "    dtype=torch.float\n",
    ")\n",
    "print(\"Loss d'un modèle aléatoire :\", loss_ce(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7477c273",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Paramétrage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1316ed00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:33.016511Z",
     "start_time": "2022-06-26T08:27:32.740265Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63d6e414",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:45.847315Z",
     "start_time": "2022-06-26T08:27:39.218927Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0626 08:27:39.242717 140151762241344 modeling.py:577] loading archive file data/inputs/models/bert-base-uncased\n",
      "I0626 08:27:39.248798 140151762241344 modeling.py:598] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0626 08:27:45.839415 140151762241344 modeling.py:648] Weights of BertForTokenClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0626 08:27:45.840662 140151762241344 modeling.py:651] Weights from pretrained model not used in BertForTokenClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model = EntityModel(num_tag=num_tag)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1419d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:45.872213Z",
     "start_time": "2022-06-26T08:27:45.849574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01ce6735",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:27:59.345025Z",
     "start_time": "2022-06-26T08:27:59.322605Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36b83b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:28:01.064330Z",
     "start_time": "2022-06-26T08:28:01.040905Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3597 batchs vont être envoyés dans le réseau au cours de 4 epochs.\n"
     ]
    }
   ],
   "source": [
    "num_train_steps = int(len(sentences_minitrain) / PARAMS.MODEL.TRAIN_BATCH_SIZE * PARAMS.MODEL.EPOCHS)\n",
    "print(f\"{num_train_steps} batchs vont être envoyés dans le réseau au cours de {PARAMS.MODEL.EPOCHS} epochs.\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(optimizer_parameters, lr=PARAMS.MODEL.LEARNING_RATE)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de28a81",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3fdea33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T16:03:39.204317Z",
     "start_time": "2022-06-06T14:32:25.865579Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b227ec753d54a69914756a8f87e88f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch #0 : loss = 1.175908\n",
      "Batch #10 : loss = 0.328072\n",
      "Batch #20 : loss = 0.349201\n",
      "Batch #30 : loss = 0.235481\n",
      "Batch #40 : loss = 0.197290\n",
      "Batch #50 : loss = 0.197902\n",
      "Batch #60 : loss = 0.246846\n",
      "Batch #70 : loss = 0.204052\n",
      "Batch #80 : loss = 0.193391\n",
      "Batch #90 : loss = 0.185136\n",
      "Batch #100 : loss = 0.261966\n",
      "Batch #110 : loss = 0.199818\n",
      "Batch #120 : loss = 0.200901\n",
      "Batch #130 : loss = 0.189416\n",
      "Batch #140 : loss = 0.242348\n",
      "Batch #150 : loss = 0.256955\n",
      "Batch #160 : loss = 0.189808\n",
      "Batch #170 : loss = 0.190499\n",
      "Batch #180 : loss = 0.181705\n",
      "Batch #190 : loss = 0.178590\n",
      "Batch #200 : loss = 0.239938\n",
      "Batch #210 : loss = 0.171416\n",
      "Batch #220 : loss = 0.171301\n",
      "Batch #230 : loss = 0.213909\n",
      "Batch #240 : loss = 0.171190\n",
      "Batch #250 : loss = 0.206733\n",
      "Batch #260 : loss = 0.206354\n",
      "Batch #270 : loss = 0.199991\n",
      "Batch #280 : loss = 0.230558\n",
      "Batch #290 : loss = 0.177015\n",
      "Batch #300 : loss = 0.182233\n",
      "Batch #310 : loss = 0.185148\n",
      "Batch #320 : loss = 0.208867\n",
      "Batch #330 : loss = 0.207769\n",
      "Batch #340 : loss = 0.174035\n",
      "Batch #350 : loss = 0.165441\n",
      "Batch #360 : loss = 0.162124\n",
      "Batch #370 : loss = 0.220207\n",
      "Batch #380 : loss = 0.196012\n",
      "Batch #390 : loss = 0.206274\n",
      "Batch #400 : loss = 0.220471\n",
      "Batch #410 : loss = 0.208452\n",
      "Batch #420 : loss = 0.174205\n",
      "Batch #430 : loss = 0.160126\n",
      "Batch #440 : loss = 0.232773\n",
      "Batch #450 : loss = 0.199746\n",
      "Batch #460 : loss = 0.177220\n",
      "Batch #470 : loss = 0.184640\n",
      "Batch #480 : loss = 0.196188\n",
      "Batch #490 : loss = 0.188578\n",
      "Batch #500 : loss = 0.162318\n",
      "Batch #510 : loss = 0.189243\n",
      "Batch #520 : loss = 0.189442\n",
      "Batch #530 : loss = 0.170462\n",
      "Batch #540 : loss = 0.161368\n",
      "Batch #550 : loss = 0.192996\n",
      "Batch #560 : loss = 0.224467\n",
      "Batch #570 : loss = 0.222384\n",
      "Batch #580 : loss = 0.186831\n",
      "Batch #590 : loss = 0.186662\n",
      "Batch #600 : loss = 0.200299\n",
      "Batch #610 : loss = 0.172068\n",
      "Batch #620 : loss = 0.154005\n",
      "Batch #630 : loss = 0.213737\n",
      "Batch #640 : loss = 0.192606\n",
      "Batch #650 : loss = 0.187046\n",
      "Batch #660 : loss = 0.210072\n",
      "Batch #670 : loss = 0.212209\n",
      "Batch #680 : loss = 0.201198\n",
      "Batch #690 : loss = 0.196279\n",
      "Batch #700 : loss = 0.209558\n",
      "Batch #710 : loss = 0.177314\n",
      "Batch #720 : loss = 0.152593\n",
      "Batch #730 : loss = 0.176856\n",
      "Batch #740 : loss = 0.202382\n",
      "Batch #750 : loss = 0.169101\n",
      "Batch #760 : loss = 0.210733\n",
      "Batch #770 : loss = 0.161795\n",
      "Batch #780 : loss = 0.158555\n",
      "Batch #790 : loss = 0.190354\n",
      "Batch #800 : loss = 0.150335\n",
      "Batch #810 : loss = 0.190535\n",
      "Batch #820 : loss = 0.204372\n",
      "Batch #830 : loss = 0.189985\n",
      "Batch #840 : loss = 0.193512\n",
      "Batch #850 : loss = 0.157112\n",
      "Batch #860 : loss = 0.223223\n",
      "Batch #870 : loss = 0.149107\n",
      "Batch #880 : loss = 0.161060\n",
      "Batch #890 : loss = 0.206928\n",
      "Train Loss = 0.20080046702590254 Valid Loss = 0.060834062732756135\n",
      "Batch #0 : loss = 0.154442\n",
      "Batch #10 : loss = 0.204021\n",
      "Batch #20 : loss = 0.190550\n",
      "Batch #30 : loss = 0.189061\n",
      "Batch #40 : loss = 0.172101\n",
      "Batch #50 : loss = 0.135968\n",
      "Batch #60 : loss = 0.185770\n",
      "Batch #70 : loss = 0.169418\n",
      "Batch #80 : loss = 0.125327\n",
      "Batch #90 : loss = 0.170707\n",
      "Batch #100 : loss = 0.196219\n",
      "Batch #110 : loss = 0.157618\n",
      "Batch #120 : loss = 0.182315\n",
      "Batch #130 : loss = 0.137744\n",
      "Batch #140 : loss = 0.155457\n",
      "Batch #150 : loss = 0.213429\n",
      "Batch #160 : loss = 0.194920\n",
      "Batch #170 : loss = 0.141323\n",
      "Batch #180 : loss = 0.156925\n",
      "Batch #190 : loss = 0.160971\n",
      "Batch #200 : loss = 0.167195\n",
      "Batch #210 : loss = 0.132459\n",
      "Batch #220 : loss = 0.155706\n",
      "Batch #230 : loss = 0.157784\n",
      "Batch #240 : loss = 0.174519\n",
      "Batch #250 : loss = 0.229977\n",
      "Batch #260 : loss = 0.171997\n",
      "Batch #270 : loss = 0.187064\n",
      "Batch #280 : loss = 0.215706\n",
      "Batch #290 : loss = 0.148094\n",
      "Batch #300 : loss = 0.168275\n",
      "Batch #310 : loss = 0.157143\n",
      "Batch #320 : loss = 0.211530\n",
      "Batch #330 : loss = 0.162026\n",
      "Batch #340 : loss = 0.144152\n",
      "Batch #350 : loss = 0.135141\n",
      "Batch #360 : loss = 0.160952\n",
      "Batch #370 : loss = 0.201715\n",
      "Batch #380 : loss = 0.187022\n",
      "Batch #390 : loss = 0.166540\n",
      "Batch #400 : loss = 0.187277\n",
      "Batch #410 : loss = 0.185798\n",
      "Batch #420 : loss = 0.164530\n",
      "Batch #430 : loss = 0.143946\n",
      "Batch #440 : loss = 0.240537\n",
      "Batch #450 : loss = 0.185325\n",
      "Batch #460 : loss = 0.146627\n",
      "Batch #470 : loss = 0.173532\n",
      "Batch #480 : loss = 0.143850\n",
      "Batch #490 : loss = 0.166989\n",
      "Batch #500 : loss = 0.141308\n",
      "Batch #510 : loss = 0.162456\n",
      "Batch #520 : loss = 0.196210\n",
      "Batch #530 : loss = 0.153698\n",
      "Batch #540 : loss = 0.145784\n",
      "Batch #550 : loss = 0.164318\n",
      "Batch #560 : loss = 0.149505\n",
      "Batch #570 : loss = 0.181145\n",
      "Batch #580 : loss = 0.159125\n",
      "Batch #590 : loss = 0.151850\n",
      "Batch #600 : loss = 0.168436\n",
      "Batch #610 : loss = 0.176217\n",
      "Batch #620 : loss = 0.147578\n",
      "Batch #630 : loss = 0.183671\n",
      "Batch #640 : loss = 0.173648\n",
      "Batch #650 : loss = 0.193530\n",
      "Batch #660 : loss = 0.158855\n",
      "Batch #670 : loss = 0.171043\n",
      "Batch #680 : loss = 0.167219\n",
      "Batch #690 : loss = 0.160131\n",
      "Batch #700 : loss = 0.197138\n",
      "Batch #710 : loss = 0.158412\n",
      "Batch #720 : loss = 0.160637\n",
      "Batch #730 : loss = 0.160244\n",
      "Batch #740 : loss = 0.171512\n",
      "Batch #750 : loss = 0.129792\n",
      "Batch #760 : loss = 0.175892\n",
      "Batch #770 : loss = 0.144855\n",
      "Batch #780 : loss = 0.151051\n",
      "Batch #790 : loss = 0.152919\n",
      "Batch #800 : loss = 0.149606\n",
      "Batch #810 : loss = 0.174320\n",
      "Batch #820 : loss = 0.158633\n",
      "Batch #830 : loss = 0.182498\n",
      "Batch #840 : loss = 0.205985\n",
      "Batch #850 : loss = 0.153063\n",
      "Batch #860 : loss = 0.223946\n",
      "Batch #870 : loss = 0.154884\n",
      "Batch #880 : loss = 0.141013\n",
      "Batch #890 : loss = 0.185907\n",
      "Train Loss = 0.16704608992569978 Valid Loss = 0.06263734875557324\n",
      "Batch #0 : loss = 0.158183\n",
      "Batch #10 : loss = 0.184247\n",
      "Batch #20 : loss = 0.175167\n",
      "Batch #30 : loss = 0.157497\n",
      "Batch #40 : loss = 0.137721\n",
      "Batch #50 : loss = 0.124749\n",
      "Batch #60 : loss = 0.171914\n",
      "Batch #70 : loss = 0.140793\n",
      "Batch #80 : loss = 0.153739\n",
      "Batch #90 : loss = 0.143370\n",
      "Batch #100 : loss = 0.159425\n",
      "Batch #110 : loss = 0.148536\n",
      "Batch #120 : loss = 0.168208\n",
      "Batch #130 : loss = 0.117889\n",
      "Batch #140 : loss = 0.150496\n",
      "Batch #150 : loss = 0.195458\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-f89fcf999737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     train_loss = train_fn(minitrain_data_loader, model, optimizer,\n\u001b[0;32m----> 6\u001b[0;31m                           device, scheduler, pbar=pbar, num_epoch=num_epoch)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# On évalue le modèle à la fin de chaque epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dataml/LABTA/MRC_hbr/Pierre/PFPR/NER-code/ner_pytorch/engine.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(data_loader, model, optimizer, device, scheduler, pbar, num_epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mbatch_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/dataml/LABTA/MRC_hbr/Pierre/PFPR/NER-code/ner_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ids, mask, token_type_ids, target_tag)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mo1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mbo_tag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# tag = self.out_tag(bo_tag)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = np.inf\n",
    "\n",
    "pbar = tqdm(range(PARAMS.MODEL.EPOCHS))\n",
    "for num_epoch, epoch in enumerate(pbar):\n",
    "    train_loss = train_fn(minitrain_data_loader, model, optimizer,\n",
    "                          device, scheduler, pbar=pbar, num_epoch=num_epoch)\n",
    "    \n",
    "    # On évalue le modèle à la fin de chaque epoch\n",
    "    test_loss = eval_fn(valid_data_loader, model, device)\n",
    "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "    \n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), PARAMS.PATHS.MODEL_SAVED)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eecefab4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Train Loss = 0.2257268762588501 Valid Loss = 0.07416860982775689\n",
    "Train Loss = 0.18738604148228963 Valid Loss = 0.06755437478423118\n",
    "Train Loss = 0.1802095964219835 Valid Loss = 0.06718131552139918\n",
    "Train Loss = 0.17476963301499684 Valid Loss = 0.06649512027700742\n",
    "Train Loss = 0.17063462575276692 Valid Loss = 0.06719479302565257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4644ce",
   "metadata": {},
   "source": [
    "## Prédictions\n",
    "\n",
    "Utilisons le modèle finetuné pour prédire une nouvelle phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d26c821",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T09:10:23.072443Z",
     "start_time": "2022-07-11T09:10:16.147273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../models/camembert-base were not used when initializing CamembertForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForTokenClassification were not initialized from the model checkpoint at ../models/camembert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EntityModel:\n\tMissing key(s) in state_dict: \"bert.roberta.embeddings.position_ids\", \"bert.roberta.embeddings.word_embeddings.weight\", \"bert.roberta.embeddings.position_embeddings.weight\", \"bert.roberta.embeddings.token_type_embeddings.weight\", \"bert.roberta.embeddings.LayerNorm.weight\", \"bert.roberta.embeddings.LayerNorm.bias\", \"bert.roberta.encoder.layer.0.attention.self.query.weight\", \"bert.roberta.encoder.layer.0.attention.self.query.bias\", \"bert.roberta.encoder.layer.0.attention.self.key.weight\", \"bert.roberta.encoder.layer.0.attention.self.key.bias\", \"bert.roberta.encoder.layer.0.attention.self.value.weight\", \"bert.roberta.encoder.layer.0.attention.self.value.bias\", \"bert.roberta.encoder.layer.0.attention.output.dense.weight\", \"bert.roberta.encoder.layer.0.attention.output.dense.bias\", \"bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.0.intermediate.dense.weight\", \"bert.roberta.encoder.layer.0.intermediate.dense.bias\", \"bert.roberta.encoder.layer.0.output.dense.weight\", \"bert.roberta.encoder.layer.0.output.dense.bias\", \"bert.roberta.encoder.layer.0.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.0.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.1.attention.self.query.weight\", \"bert.roberta.encoder.layer.1.attention.self.query.bias\", \"bert.roberta.encoder.layer.1.attention.self.key.weight\", \"bert.roberta.encoder.layer.1.attention.self.key.bias\", \"bert.roberta.encoder.layer.1.attention.self.value.weight\", \"bert.roberta.encoder.layer.1.attention.self.value.bias\", \"bert.roberta.encoder.layer.1.attention.output.dense.weight\", \"bert.roberta.encoder.layer.1.attention.output.dense.bias\", \"bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.1.intermediate.dense.weight\", \"bert.roberta.encoder.layer.1.intermediate.dense.bias\", \"bert.roberta.encoder.layer.1.output.dense.weight\", \"bert.roberta.encoder.layer.1.output.dense.bias\", \"bert.roberta.encoder.layer.1.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.1.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.2.attention.self.query.weight\", \"bert.roberta.encoder.layer.2.attention.self.query.bias\", \"bert.roberta.encoder.layer.2.attention.self.key.weight\", \"bert.roberta.encoder.layer.2.attention.self.key.bias\", \"bert.roberta.encoder.layer.2.attention.self.value.weight\", \"bert.roberta.encoder.layer.2.attention.self.value.bias\", \"bert.roberta.encoder.layer.2.attention.output.dense.weight\", \"bert.roberta.encoder.layer.2.attention.output.dense.bias\", \"bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.2.intermediate.dense.weight\", \"bert.roberta.encoder.layer.2.intermediate.dense.bias\", \"bert.roberta.encoder.layer.2.output.dense.weight\", \"bert.roberta.encoder.layer.2.output.dense.bias\", \"bert.roberta.encoder.layer.2.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.2.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.3.attention.self.query.weight\", \"bert.roberta.encoder.layer.3.attention.self.query.bias\", \"bert.roberta.encoder.layer.3.attention.self.key.weight\", \"bert.roberta.encoder.layer.3.attention.self.key.bias\", \"bert.roberta.encoder.layer.3.attention.self.value.weight\", \"bert.roberta.encoder.layer.3.attention.self.value.bias\", \"bert.roberta.encoder.layer.3.attention.output.dense.weight\", \"bert.roberta.encoder.layer.3.attention.output.dense.bias\", \"bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.3.intermediate.dense.weight\", \"bert.roberta.encoder.layer.3.intermediate.dense.bias\", \"bert.roberta.encoder.layer.3.output.dense.weight\", \"bert.roberta.encoder.layer.3.output.dense.bias\", \"bert.roberta.encoder.layer.3.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.3.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.4.attention.self.query.weight\", \"bert.roberta.encoder.layer.4.attention.self.query.bias\", \"bert.roberta.encoder.layer.4.attention.self.key.weight\", \"bert.roberta.encoder.layer.4.attention.self.key.bias\", \"bert.roberta.encoder.layer.4.attention.self.value.weight\", \"bert.roberta.encoder.layer.4.attention.self.value.bias\", \"bert.roberta.encoder.layer.4.attention.output.dense.weight\", \"bert.roberta.encoder.layer.4.attention.output.dense.bias\", \"bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.4.intermediate.dense.weight\", \"bert.roberta.encoder.layer.4.intermediate.dense.bias\", \"bert.roberta.encoder.layer.4.output.dense.weight\", \"bert.roberta.encoder.layer.4.output.dense.bias\", \"bert.roberta.encoder.layer.4.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.4.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.5.attention.self.query.weight\", \"bert.roberta.encoder.layer.5.attention.self.query.bias\", \"bert.roberta.encoder.layer.5.attention.self.key.weight\", \"bert.roberta.encoder.layer.5.attention.self.key.bias\", \"bert.roberta.encoder.layer.5.attention.self.value.weight\", \"bert.roberta.encoder.layer.5.attention.self.value.bias\", \"bert.roberta.encoder.layer.5.attention.output.dense.weight\", \"bert.roberta.encoder.layer.5.attention.output.dense.bias\", \"bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.5.intermediate.dense.weight\", \"bert.roberta.encoder.layer.5.intermediate.dense.bias\", \"bert.roberta.encoder.layer.5.output.dense.weight\", \"bert.roberta.encoder.layer.5.output.dense.bias\", \"bert.roberta.encoder.layer.5.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.5.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.6.attention.self.query.weight\", \"bert.roberta.encoder.layer.6.attention.self.query.bias\", \"bert.roberta.encoder.layer.6.attention.self.key.weight\", \"bert.roberta.encoder.layer.6.attention.self.key.bias\", \"bert.roberta.encoder.layer.6.attention.self.value.weight\", \"bert.roberta.encoder.layer.6.attention.self.value.bias\", \"bert.roberta.encoder.layer.6.attention.output.dense.weight\", \"bert.roberta.encoder.layer.6.attention.output.dense.bias\", \"bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.6.intermediate.dense.weight\", \"bert.roberta.encoder.layer.6.intermediate.dense.bias\", \"bert.roberta.encoder.layer.6.output.dense.weight\", \"bert.roberta.encoder.layer.6.output.dense.bias\", \"bert.roberta.encoder.layer.6.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.6.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.7.attention.self.query.weight\", \"bert.roberta.encoder.layer.7.attention.self.query.bias\", \"bert.roberta.encoder.layer.7.attention.self.key.weight\", \"bert.roberta.encoder.layer.7.attention.self.key.bias\", \"bert.roberta.encoder.layer.7.attention.self.value.weight\", \"bert.roberta.encoder.layer.7.attention.self.value.bias\", \"bert.roberta.encoder.layer.7.attention.output.dense.weight\", \"bert.roberta.encoder.layer.7.attention.output.dense.bias\", \"bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.7.intermediate.dense.weight\", \"bert.roberta.encoder.layer.7.intermediate.dense.bias\", \"bert.roberta.encoder.layer.7.output.dense.weight\", \"bert.roberta.encoder.layer.7.output.dense.bias\", \"bert.roberta.encoder.layer.7.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.7.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.8.attention.self.query.weight\", \"bert.roberta.encoder.layer.8.attention.self.query.bias\", \"bert.roberta.encoder.layer.8.attention.self.key.weight\", \"bert.roberta.encoder.layer.8.attention.self.key.bias\", \"bert.roberta.encoder.layer.8.attention.self.value.weight\", \"bert.roberta.encoder.layer.8.attention.self.value.bias\", \"bert.roberta.encoder.layer.8.attention.output.dense.weight\", \"bert.roberta.encoder.layer.8.attention.output.dense.bias\", \"bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.8.intermediate.dense.weight\", \"bert.roberta.encoder.layer.8.intermediate.dense.bias\", \"bert.roberta.encoder.layer.8.output.dense.weight\", \"bert.roberta.encoder.layer.8.output.dense.bias\", \"bert.roberta.encoder.layer.8.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.8.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.9.attention.self.query.weight\", \"bert.roberta.encoder.layer.9.attention.self.query.bias\", \"bert.roberta.encoder.layer.9.attention.self.key.weight\", \"bert.roberta.encoder.layer.9.attention.self.key.bias\", \"bert.roberta.encoder.layer.9.attention.self.value.weight\", \"bert.roberta.encoder.layer.9.attention.self.value.bias\", \"bert.roberta.encoder.layer.9.attention.output.dense.weight\", \"bert.roberta.encoder.layer.9.attention.output.dense.bias\", \"bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.9.intermediate.dense.weight\", \"bert.roberta.encoder.layer.9.intermediate.dense.bias\", \"bert.roberta.encoder.layer.9.output.dense.weight\", \"bert.roberta.encoder.layer.9.output.dense.bias\", \"bert.roberta.encoder.layer.9.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.9.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.10.attention.self.query.weight\", \"bert.roberta.encoder.layer.10.attention.self.query.bias\", \"bert.roberta.encoder.layer.10.attention.self.key.weight\", \"bert.roberta.encoder.layer.10.attention.self.key.bias\", \"bert.roberta.encoder.layer.10.attention.self.value.weight\", \"bert.roberta.encoder.layer.10.attention.self.value.bias\", \"bert.roberta.encoder.layer.10.attention.output.dense.weight\", \"bert.roberta.encoder.layer.10.attention.output.dense.bias\", \"bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.10.intermediate.dense.weight\", \"bert.roberta.encoder.layer.10.intermediate.dense.bias\", \"bert.roberta.encoder.layer.10.output.dense.weight\", \"bert.roberta.encoder.layer.10.output.dense.bias\", \"bert.roberta.encoder.layer.10.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.10.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.11.attention.self.query.weight\", \"bert.roberta.encoder.layer.11.attention.self.query.bias\", \"bert.roberta.encoder.layer.11.attention.self.key.weight\", \"bert.roberta.encoder.layer.11.attention.self.key.bias\", \"bert.roberta.encoder.layer.11.attention.self.value.weight\", \"bert.roberta.encoder.layer.11.attention.self.value.bias\", \"bert.roberta.encoder.layer.11.attention.output.dense.weight\", \"bert.roberta.encoder.layer.11.attention.output.dense.bias\", \"bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.11.intermediate.dense.weight\", \"bert.roberta.encoder.layer.11.intermediate.dense.bias\", \"bert.roberta.encoder.layer.11.output.dense.weight\", \"bert.roberta.encoder.layer.11.output.dense.bias\", \"bert.roberta.encoder.layer.11.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.11.output.LayerNorm.bias\". \n\tUnexpected key(s) in state_dict: \"bert.bert.embeddings.word_embeddings.weight\", \"bert.bert.embeddings.position_embeddings.weight\", \"bert.bert.embeddings.token_type_embeddings.weight\", \"bert.bert.embeddings.LayerNorm.weight\", \"bert.bert.embeddings.LayerNorm.bias\", \"bert.bert.encoder.layer.0.attention.self.query.weight\", \"bert.bert.encoder.layer.0.attention.self.query.bias\", \"bert.bert.encoder.layer.0.attention.self.key.weight\", \"bert.bert.encoder.layer.0.attention.self.key.bias\", \"bert.bert.encoder.layer.0.attention.self.value.weight\", \"bert.bert.encoder.layer.0.attention.self.value.bias\", \"bert.bert.encoder.layer.0.attention.output.dense.weight\", \"bert.bert.encoder.layer.0.attention.output.dense.bias\", \"bert.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.0.intermediate.dense.weight\", \"bert.bert.encoder.layer.0.intermediate.dense.bias\", \"bert.bert.encoder.layer.0.output.dense.weight\", \"bert.bert.encoder.layer.0.output.dense.bias\", \"bert.bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.bert.encoder.layer.1.attention.self.query.weight\", \"bert.bert.encoder.layer.1.attention.self.query.bias\", \"bert.bert.encoder.layer.1.attention.self.key.weight\", \"bert.bert.encoder.layer.1.attention.self.key.bias\", \"bert.bert.encoder.layer.1.attention.self.value.weight\", \"bert.bert.encoder.layer.1.attention.self.value.bias\", \"bert.bert.encoder.layer.1.attention.output.dense.weight\", \"bert.bert.encoder.layer.1.attention.output.dense.bias\", \"bert.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.1.intermediate.dense.weight\", \"bert.bert.encoder.layer.1.intermediate.dense.bias\", \"bert.bert.encoder.layer.1.output.dense.weight\", \"bert.bert.encoder.layer.1.output.dense.bias\", \"bert.bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.bert.encoder.layer.2.attention.self.query.weight\", \"bert.bert.encoder.layer.2.attention.self.query.bias\", \"bert.bert.encoder.layer.2.attention.self.key.weight\", \"bert.bert.encoder.layer.2.attention.self.key.bias\", \"bert.bert.encoder.layer.2.attention.self.value.weight\", \"bert.bert.encoder.layer.2.attention.self.value.bias\", \"bert.bert.encoder.layer.2.attention.output.dense.weight\", \"bert.bert.encoder.layer.2.attention.output.dense.bias\", \"bert.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.2.intermediate.dense.weight\", \"bert.bert.encoder.layer.2.intermediate.dense.bias\", \"bert.bert.encoder.layer.2.output.dense.weight\", \"bert.bert.encoder.layer.2.output.dense.bias\", \"bert.bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.bert.encoder.layer.3.attention.self.query.weight\", \"bert.bert.encoder.layer.3.attention.self.query.bias\", \"bert.bert.encoder.layer.3.attention.self.key.weight\", \"bert.bert.encoder.layer.3.attention.self.key.bias\", \"bert.bert.encoder.layer.3.attention.self.value.weight\", \"bert.bert.encoder.layer.3.attention.self.value.bias\", \"bert.bert.encoder.layer.3.attention.output.dense.weight\", \"bert.bert.encoder.layer.3.attention.output.dense.bias\", \"bert.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.3.intermediate.dense.weight\", \"bert.bert.encoder.layer.3.intermediate.dense.bias\", \"bert.bert.encoder.layer.3.output.dense.weight\", \"bert.bert.encoder.layer.3.output.dense.bias\", \"bert.bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.bert.encoder.layer.4.attention.self.query.weight\", \"bert.bert.encoder.layer.4.attention.self.query.bias\", \"bert.bert.encoder.layer.4.attention.self.key.weight\", \"bert.bert.encoder.layer.4.attention.self.key.bias\", \"bert.bert.encoder.layer.4.attention.self.value.weight\", \"bert.bert.encoder.layer.4.attention.self.value.bias\", \"bert.bert.encoder.layer.4.attention.output.dense.weight\", \"bert.bert.encoder.layer.4.attention.output.dense.bias\", \"bert.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.4.intermediate.dense.weight\", \"bert.bert.encoder.layer.4.intermediate.dense.bias\", \"bert.bert.encoder.layer.4.output.dense.weight\", \"bert.bert.encoder.layer.4.output.dense.bias\", \"bert.bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.bert.encoder.layer.5.attention.self.query.weight\", \"bert.bert.encoder.layer.5.attention.self.query.bias\", \"bert.bert.encoder.layer.5.attention.self.key.weight\", \"bert.bert.encoder.layer.5.attention.self.key.bias\", \"bert.bert.encoder.layer.5.attention.self.value.weight\", \"bert.bert.encoder.layer.5.attention.self.value.bias\", \"bert.bert.encoder.layer.5.attention.output.dense.weight\", \"bert.bert.encoder.layer.5.attention.output.dense.bias\", \"bert.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.5.intermediate.dense.weight\", \"bert.bert.encoder.layer.5.intermediate.dense.bias\", \"bert.bert.encoder.layer.5.output.dense.weight\", \"bert.bert.encoder.layer.5.output.dense.bias\", \"bert.bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.bert.encoder.layer.6.attention.self.query.weight\", \"bert.bert.encoder.layer.6.attention.self.query.bias\", \"bert.bert.encoder.layer.6.attention.self.key.weight\", \"bert.bert.encoder.layer.6.attention.self.key.bias\", \"bert.bert.encoder.layer.6.attention.self.value.weight\", \"bert.bert.encoder.layer.6.attention.self.value.bias\", \"bert.bert.encoder.layer.6.attention.output.dense.weight\", \"bert.bert.encoder.layer.6.attention.output.dense.bias\", \"bert.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.6.intermediate.dense.weight\", \"bert.bert.encoder.layer.6.intermediate.dense.bias\", \"bert.bert.encoder.layer.6.output.dense.weight\", \"bert.bert.encoder.layer.6.output.dense.bias\", \"bert.bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.bert.encoder.layer.7.attention.self.query.weight\", \"bert.bert.encoder.layer.7.attention.self.query.bias\", \"bert.bert.encoder.layer.7.attention.self.key.weight\", \"bert.bert.encoder.layer.7.attention.self.key.bias\", \"bert.bert.encoder.layer.7.attention.self.value.weight\", \"bert.bert.encoder.layer.7.attention.self.value.bias\", \"bert.bert.encoder.layer.7.attention.output.dense.weight\", \"bert.bert.encoder.layer.7.attention.output.dense.bias\", \"bert.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.7.intermediate.dense.weight\", \"bert.bert.encoder.layer.7.intermediate.dense.bias\", \"bert.bert.encoder.layer.7.output.dense.weight\", \"bert.bert.encoder.layer.7.output.dense.bias\", \"bert.bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.bert.encoder.layer.8.attention.self.query.weight\", \"bert.bert.encoder.layer.8.attention.self.query.bias\", \"bert.bert.encoder.layer.8.attention.self.key.weight\", \"bert.bert.encoder.layer.8.attention.self.key.bias\", \"bert.bert.encoder.layer.8.attention.self.value.weight\", \"bert.bert.encoder.layer.8.attention.self.value.bias\", \"bert.bert.encoder.layer.8.attention.output.dense.weight\", \"bert.bert.encoder.layer.8.attention.output.dense.bias\", \"bert.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.8.intermediate.dense.weight\", \"bert.bert.encoder.layer.8.intermediate.dense.bias\", \"bert.bert.encoder.layer.8.output.dense.weight\", \"bert.bert.encoder.layer.8.output.dense.bias\", \"bert.bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.bert.encoder.layer.9.attention.self.query.weight\", \"bert.bert.encoder.layer.9.attention.self.query.bias\", \"bert.bert.encoder.layer.9.attention.self.key.weight\", \"bert.bert.encoder.layer.9.attention.self.key.bias\", \"bert.bert.encoder.layer.9.attention.self.value.weight\", \"bert.bert.encoder.layer.9.attention.self.value.bias\", \"bert.bert.encoder.layer.9.attention.output.dense.weight\", \"bert.bert.encoder.layer.9.attention.output.dense.bias\", \"bert.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.9.intermediate.dense.weight\", \"bert.bert.encoder.layer.9.intermediate.dense.bias\", \"bert.bert.encoder.layer.9.output.dense.weight\", \"bert.bert.encoder.layer.9.output.dense.bias\", \"bert.bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.bert.encoder.layer.10.attention.self.query.weight\", \"bert.bert.encoder.layer.10.attention.self.query.bias\", \"bert.bert.encoder.layer.10.attention.self.key.weight\", \"bert.bert.encoder.layer.10.attention.self.key.bias\", \"bert.bert.encoder.layer.10.attention.self.value.weight\", \"bert.bert.encoder.layer.10.attention.self.value.bias\", \"bert.bert.encoder.layer.10.attention.output.dense.weight\", \"bert.bert.encoder.layer.10.attention.output.dense.bias\", \"bert.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.10.intermediate.dense.weight\", \"bert.bert.encoder.layer.10.intermediate.dense.bias\", \"bert.bert.encoder.layer.10.output.dense.weight\", \"bert.bert.encoder.layer.10.output.dense.bias\", \"bert.bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.bert.encoder.layer.11.attention.self.query.weight\", \"bert.bert.encoder.layer.11.attention.self.query.bias\", \"bert.bert.encoder.layer.11.attention.self.key.weight\", \"bert.bert.encoder.layer.11.attention.self.key.bias\", \"bert.bert.encoder.layer.11.attention.self.value.weight\", \"bert.bert.encoder.layer.11.attention.self.value.bias\", \"bert.bert.encoder.layer.11.attention.output.dense.weight\", \"bert.bert.encoder.layer.11.attention.output.dense.bias\", \"bert.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.11.intermediate.dense.weight\", \"bert.bert.encoder.layer.11.intermediate.dense.bias\", \"bert.bert.encoder.layer.11.output.dense.weight\", \"bert.bert.encoder.layer.11.output.dense.bias\", \"bert.bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.bert.pooler.dense.weight\", \"bert.bert.pooler.dense.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-651a428938ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# On désérialise le meilleur modèle enregistré :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEntityModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_tag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/models/model_trained_en.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1407\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1408\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EntityModel:\n\tMissing key(s) in state_dict: \"bert.roberta.embeddings.position_ids\", \"bert.roberta.embeddings.word_embeddings.weight\", \"bert.roberta.embeddings.position_embeddings.weight\", \"bert.roberta.embeddings.token_type_embeddings.weight\", \"bert.roberta.embeddings.LayerNorm.weight\", \"bert.roberta.embeddings.LayerNorm.bias\", \"bert.roberta.encoder.layer.0.attention.self.query.weight\", \"bert.roberta.encoder.layer.0.attention.self.query.bias\", \"bert.roberta.encoder.layer.0.attention.self.key.weight\", \"bert.roberta.encoder.layer.0.attention.self.key.bias\", \"bert.roberta.encoder.layer.0.attention.self.value.weight\", \"bert.roberta.encoder.layer.0.attention.self.value.bias\", \"bert.roberta.encoder.layer.0.attention.output.dense.weight\", \"bert.roberta.encoder.layer.0.attention.output.dense.bias\", \"bert.roberta.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.0.intermediate.dense.weight\", \"bert.roberta.encoder.layer.0.intermediate.dense.bias\", \"bert.roberta.encoder.layer.0.output.dense.weight\", \"bert.roberta.encoder.layer.0.output.dense.bias\", \"bert.roberta.encoder.layer.0.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.0.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.1.attention.self.query.weight\", \"bert.roberta.encoder.layer.1.attention.self.query.bias\", \"bert.roberta.encoder.layer.1.attention.self.key.weight\", \"bert.roberta.encoder.layer.1.attention.self.key.bias\", \"bert.roberta.encoder.layer.1.attention.self.value.weight\", \"bert.roberta.encoder.layer.1.attention.self.value.bias\", \"bert.roberta.encoder.layer.1.attention.output.dense.weight\", \"bert.roberta.encoder.layer.1.attention.output.dense.bias\", \"bert.roberta.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.1.intermediate.dense.weight\", \"bert.roberta.encoder.layer.1.intermediate.dense.bias\", \"bert.roberta.encoder.layer.1.output.dense.weight\", \"bert.roberta.encoder.layer.1.output.dense.bias\", \"bert.roberta.encoder.layer.1.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.1.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.2.attention.self.query.weight\", \"bert.roberta.encoder.layer.2.attention.self.query.bias\", \"bert.roberta.encoder.layer.2.attention.self.key.weight\", \"bert.roberta.encoder.layer.2.attention.self.key.bias\", \"bert.roberta.encoder.layer.2.attention.self.value.weight\", \"bert.roberta.encoder.layer.2.attention.self.value.bias\", \"bert.roberta.encoder.layer.2.attention.output.dense.weight\", \"bert.roberta.encoder.layer.2.attention.output.dense.bias\", \"bert.roberta.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.2.intermediate.dense.weight\", \"bert.roberta.encoder.layer.2.intermediate.dense.bias\", \"bert.roberta.encoder.layer.2.output.dense.weight\", \"bert.roberta.encoder.layer.2.output.dense.bias\", \"bert.roberta.encoder.layer.2.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.2.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.3.attention.self.query.weight\", \"bert.roberta.encoder.layer.3.attention.self.query.bias\", \"bert.roberta.encoder.layer.3.attention.self.key.weight\", \"bert.roberta.encoder.layer.3.attention.self.key.bias\", \"bert.roberta.encoder.layer.3.attention.self.value.weight\", \"bert.roberta.encoder.layer.3.attention.self.value.bias\", \"bert.roberta.encoder.layer.3.attention.output.dense.weight\", \"bert.roberta.encoder.layer.3.attention.output.dense.bias\", \"bert.roberta.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.3.intermediate.dense.weight\", \"bert.roberta.encoder.layer.3.intermediate.dense.bias\", \"bert.roberta.encoder.layer.3.output.dense.weight\", \"bert.roberta.encoder.layer.3.output.dense.bias\", \"bert.roberta.encoder.layer.3.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.3.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.4.attention.self.query.weight\", \"bert.roberta.encoder.layer.4.attention.self.query.bias\", \"bert.roberta.encoder.layer.4.attention.self.key.weight\", \"bert.roberta.encoder.layer.4.attention.self.key.bias\", \"bert.roberta.encoder.layer.4.attention.self.value.weight\", \"bert.roberta.encoder.layer.4.attention.self.value.bias\", \"bert.roberta.encoder.layer.4.attention.output.dense.weight\", \"bert.roberta.encoder.layer.4.attention.output.dense.bias\", \"bert.roberta.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.4.intermediate.dense.weight\", \"bert.roberta.encoder.layer.4.intermediate.dense.bias\", \"bert.roberta.encoder.layer.4.output.dense.weight\", \"bert.roberta.encoder.layer.4.output.dense.bias\", \"bert.roberta.encoder.layer.4.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.4.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.5.attention.self.query.weight\", \"bert.roberta.encoder.layer.5.attention.self.query.bias\", \"bert.roberta.encoder.layer.5.attention.self.key.weight\", \"bert.roberta.encoder.layer.5.attention.self.key.bias\", \"bert.roberta.encoder.layer.5.attention.self.value.weight\", \"bert.roberta.encoder.layer.5.attention.self.value.bias\", \"bert.roberta.encoder.layer.5.attention.output.dense.weight\", \"bert.roberta.encoder.layer.5.attention.output.dense.bias\", \"bert.roberta.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.5.intermediate.dense.weight\", \"bert.roberta.encoder.layer.5.intermediate.dense.bias\", \"bert.roberta.encoder.layer.5.output.dense.weight\", \"bert.roberta.encoder.layer.5.output.dense.bias\", \"bert.roberta.encoder.layer.5.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.5.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.6.attention.self.query.weight\", \"bert.roberta.encoder.layer.6.attention.self.query.bias\", \"bert.roberta.encoder.layer.6.attention.self.key.weight\", \"bert.roberta.encoder.layer.6.attention.self.key.bias\", \"bert.roberta.encoder.layer.6.attention.self.value.weight\", \"bert.roberta.encoder.layer.6.attention.self.value.bias\", \"bert.roberta.encoder.layer.6.attention.output.dense.weight\", \"bert.roberta.encoder.layer.6.attention.output.dense.bias\", \"bert.roberta.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.6.intermediate.dense.weight\", \"bert.roberta.encoder.layer.6.intermediate.dense.bias\", \"bert.roberta.encoder.layer.6.output.dense.weight\", \"bert.roberta.encoder.layer.6.output.dense.bias\", \"bert.roberta.encoder.layer.6.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.6.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.7.attention.self.query.weight\", \"bert.roberta.encoder.layer.7.attention.self.query.bias\", \"bert.roberta.encoder.layer.7.attention.self.key.weight\", \"bert.roberta.encoder.layer.7.attention.self.key.bias\", \"bert.roberta.encoder.layer.7.attention.self.value.weight\", \"bert.roberta.encoder.layer.7.attention.self.value.bias\", \"bert.roberta.encoder.layer.7.attention.output.dense.weight\", \"bert.roberta.encoder.layer.7.attention.output.dense.bias\", \"bert.roberta.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.7.intermediate.dense.weight\", \"bert.roberta.encoder.layer.7.intermediate.dense.bias\", \"bert.roberta.encoder.layer.7.output.dense.weight\", \"bert.roberta.encoder.layer.7.output.dense.bias\", \"bert.roberta.encoder.layer.7.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.7.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.8.attention.self.query.weight\", \"bert.roberta.encoder.layer.8.attention.self.query.bias\", \"bert.roberta.encoder.layer.8.attention.self.key.weight\", \"bert.roberta.encoder.layer.8.attention.self.key.bias\", \"bert.roberta.encoder.layer.8.attention.self.value.weight\", \"bert.roberta.encoder.layer.8.attention.self.value.bias\", \"bert.roberta.encoder.layer.8.attention.output.dense.weight\", \"bert.roberta.encoder.layer.8.attention.output.dense.bias\", \"bert.roberta.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.8.intermediate.dense.weight\", \"bert.roberta.encoder.layer.8.intermediate.dense.bias\", \"bert.roberta.encoder.layer.8.output.dense.weight\", \"bert.roberta.encoder.layer.8.output.dense.bias\", \"bert.roberta.encoder.layer.8.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.8.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.9.attention.self.query.weight\", \"bert.roberta.encoder.layer.9.attention.self.query.bias\", \"bert.roberta.encoder.layer.9.attention.self.key.weight\", \"bert.roberta.encoder.layer.9.attention.self.key.bias\", \"bert.roberta.encoder.layer.9.attention.self.value.weight\", \"bert.roberta.encoder.layer.9.attention.self.value.bias\", \"bert.roberta.encoder.layer.9.attention.output.dense.weight\", \"bert.roberta.encoder.layer.9.attention.output.dense.bias\", \"bert.roberta.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.9.intermediate.dense.weight\", \"bert.roberta.encoder.layer.9.intermediate.dense.bias\", \"bert.roberta.encoder.layer.9.output.dense.weight\", \"bert.roberta.encoder.layer.9.output.dense.bias\", \"bert.roberta.encoder.layer.9.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.9.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.10.attention.self.query.weight\", \"bert.roberta.encoder.layer.10.attention.self.query.bias\", \"bert.roberta.encoder.layer.10.attention.self.key.weight\", \"bert.roberta.encoder.layer.10.attention.self.key.bias\", \"bert.roberta.encoder.layer.10.attention.self.value.weight\", \"bert.roberta.encoder.layer.10.attention.self.value.bias\", \"bert.roberta.encoder.layer.10.attention.output.dense.weight\", \"bert.roberta.encoder.layer.10.attention.output.dense.bias\", \"bert.roberta.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.10.intermediate.dense.weight\", \"bert.roberta.encoder.layer.10.intermediate.dense.bias\", \"bert.roberta.encoder.layer.10.output.dense.weight\", \"bert.roberta.encoder.layer.10.output.dense.bias\", \"bert.roberta.encoder.layer.10.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.10.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.11.attention.self.query.weight\", \"bert.roberta.encoder.layer.11.attention.self.query.bias\", \"bert.roberta.encoder.layer.11.attention.self.key.weight\", \"bert.roberta.encoder.layer.11.attention.self.key.bias\", \"bert.roberta.encoder.layer.11.attention.self.value.weight\", \"bert.roberta.encoder.layer.11.attention.self.value.bias\", \"bert.roberta.encoder.layer.11.attention.output.dense.weight\", \"bert.roberta.encoder.layer.11.attention.output.dense.bias\", \"bert.roberta.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.roberta.encoder.layer.11.intermediate.dense.weight\", \"bert.roberta.encoder.layer.11.intermediate.dense.bias\", \"bert.roberta.encoder.layer.11.output.dense.weight\", \"bert.roberta.encoder.layer.11.output.dense.bias\", \"bert.roberta.encoder.layer.11.output.LayerNorm.weight\", \"bert.roberta.encoder.layer.11.output.LayerNorm.bias\". \n\tUnexpected key(s) in state_dict: \"bert.bert.embeddings.word_embeddings.weight\", \"bert.bert.embeddings.position_embeddings.weight\", \"bert.bert.embeddings.token_type_embeddings.weight\", \"bert.bert.embeddings.LayerNorm.weight\", \"bert.bert.embeddings.LayerNorm.bias\", \"bert.bert.encoder.layer.0.attention.self.query.weight\", \"bert.bert.encoder.layer.0.attention.self.query.bias\", \"bert.bert.encoder.layer.0.attention.self.key.weight\", \"bert.bert.encoder.layer.0.attention.self.key.bias\", \"bert.bert.encoder.layer.0.attention.self.value.weight\", \"bert.bert.encoder.layer.0.attention.self.value.bias\", \"bert.bert.encoder.layer.0.attention.output.dense.weight\", \"bert.bert.encoder.layer.0.attention.output.dense.bias\", \"bert.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.0.intermediate.dense.weight\", \"bert.bert.encoder.layer.0.intermediate.dense.bias\", \"bert.bert.encoder.layer.0.output.dense.weight\", \"bert.bert.encoder.layer.0.output.dense.bias\", \"bert.bert.encoder.layer.0.output.LayerNorm.weight\", \"bert.bert.encoder.layer.0.output.LayerNorm.bias\", \"bert.bert.encoder.layer.1.attention.self.query.weight\", \"bert.bert.encoder.layer.1.attention.self.query.bias\", \"bert.bert.encoder.layer.1.attention.self.key.weight\", \"bert.bert.encoder.layer.1.attention.self.key.bias\", \"bert.bert.encoder.layer.1.attention.self.value.weight\", \"bert.bert.encoder.layer.1.attention.self.value.bias\", \"bert.bert.encoder.layer.1.attention.output.dense.weight\", \"bert.bert.encoder.layer.1.attention.output.dense.bias\", \"bert.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.1.intermediate.dense.weight\", \"bert.bert.encoder.layer.1.intermediate.dense.bias\", \"bert.bert.encoder.layer.1.output.dense.weight\", \"bert.bert.encoder.layer.1.output.dense.bias\", \"bert.bert.encoder.layer.1.output.LayerNorm.weight\", \"bert.bert.encoder.layer.1.output.LayerNorm.bias\", \"bert.bert.encoder.layer.2.attention.self.query.weight\", \"bert.bert.encoder.layer.2.attention.self.query.bias\", \"bert.bert.encoder.layer.2.attention.self.key.weight\", \"bert.bert.encoder.layer.2.attention.self.key.bias\", \"bert.bert.encoder.layer.2.attention.self.value.weight\", \"bert.bert.encoder.layer.2.attention.self.value.bias\", \"bert.bert.encoder.layer.2.attention.output.dense.weight\", \"bert.bert.encoder.layer.2.attention.output.dense.bias\", \"bert.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.2.intermediate.dense.weight\", \"bert.bert.encoder.layer.2.intermediate.dense.bias\", \"bert.bert.encoder.layer.2.output.dense.weight\", \"bert.bert.encoder.layer.2.output.dense.bias\", \"bert.bert.encoder.layer.2.output.LayerNorm.weight\", \"bert.bert.encoder.layer.2.output.LayerNorm.bias\", \"bert.bert.encoder.layer.3.attention.self.query.weight\", \"bert.bert.encoder.layer.3.attention.self.query.bias\", \"bert.bert.encoder.layer.3.attention.self.key.weight\", \"bert.bert.encoder.layer.3.attention.self.key.bias\", \"bert.bert.encoder.layer.3.attention.self.value.weight\", \"bert.bert.encoder.layer.3.attention.self.value.bias\", \"bert.bert.encoder.layer.3.attention.output.dense.weight\", \"bert.bert.encoder.layer.3.attention.output.dense.bias\", \"bert.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.3.intermediate.dense.weight\", \"bert.bert.encoder.layer.3.intermediate.dense.bias\", \"bert.bert.encoder.layer.3.output.dense.weight\", \"bert.bert.encoder.layer.3.output.dense.bias\", \"bert.bert.encoder.layer.3.output.LayerNorm.weight\", \"bert.bert.encoder.layer.3.output.LayerNorm.bias\", \"bert.bert.encoder.layer.4.attention.self.query.weight\", \"bert.bert.encoder.layer.4.attention.self.query.bias\", \"bert.bert.encoder.layer.4.attention.self.key.weight\", \"bert.bert.encoder.layer.4.attention.self.key.bias\", \"bert.bert.encoder.layer.4.attention.self.value.weight\", \"bert.bert.encoder.layer.4.attention.self.value.bias\", \"bert.bert.encoder.layer.4.attention.output.dense.weight\", \"bert.bert.encoder.layer.4.attention.output.dense.bias\", \"bert.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.4.intermediate.dense.weight\", \"bert.bert.encoder.layer.4.intermediate.dense.bias\", \"bert.bert.encoder.layer.4.output.dense.weight\", \"bert.bert.encoder.layer.4.output.dense.bias\", \"bert.bert.encoder.layer.4.output.LayerNorm.weight\", \"bert.bert.encoder.layer.4.output.LayerNorm.bias\", \"bert.bert.encoder.layer.5.attention.self.query.weight\", \"bert.bert.encoder.layer.5.attention.self.query.bias\", \"bert.bert.encoder.layer.5.attention.self.key.weight\", \"bert.bert.encoder.layer.5.attention.self.key.bias\", \"bert.bert.encoder.layer.5.attention.self.value.weight\", \"bert.bert.encoder.layer.5.attention.self.value.bias\", \"bert.bert.encoder.layer.5.attention.output.dense.weight\", \"bert.bert.encoder.layer.5.attention.output.dense.bias\", \"bert.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.5.intermediate.dense.weight\", \"bert.bert.encoder.layer.5.intermediate.dense.bias\", \"bert.bert.encoder.layer.5.output.dense.weight\", \"bert.bert.encoder.layer.5.output.dense.bias\", \"bert.bert.encoder.layer.5.output.LayerNorm.weight\", \"bert.bert.encoder.layer.5.output.LayerNorm.bias\", \"bert.bert.encoder.layer.6.attention.self.query.weight\", \"bert.bert.encoder.layer.6.attention.self.query.bias\", \"bert.bert.encoder.layer.6.attention.self.key.weight\", \"bert.bert.encoder.layer.6.attention.self.key.bias\", \"bert.bert.encoder.layer.6.attention.self.value.weight\", \"bert.bert.encoder.layer.6.attention.self.value.bias\", \"bert.bert.encoder.layer.6.attention.output.dense.weight\", \"bert.bert.encoder.layer.6.attention.output.dense.bias\", \"bert.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.6.intermediate.dense.weight\", \"bert.bert.encoder.layer.6.intermediate.dense.bias\", \"bert.bert.encoder.layer.6.output.dense.weight\", \"bert.bert.encoder.layer.6.output.dense.bias\", \"bert.bert.encoder.layer.6.output.LayerNorm.weight\", \"bert.bert.encoder.layer.6.output.LayerNorm.bias\", \"bert.bert.encoder.layer.7.attention.self.query.weight\", \"bert.bert.encoder.layer.7.attention.self.query.bias\", \"bert.bert.encoder.layer.7.attention.self.key.weight\", \"bert.bert.encoder.layer.7.attention.self.key.bias\", \"bert.bert.encoder.layer.7.attention.self.value.weight\", \"bert.bert.encoder.layer.7.attention.self.value.bias\", \"bert.bert.encoder.layer.7.attention.output.dense.weight\", \"bert.bert.encoder.layer.7.attention.output.dense.bias\", \"bert.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.7.intermediate.dense.weight\", \"bert.bert.encoder.layer.7.intermediate.dense.bias\", \"bert.bert.encoder.layer.7.output.dense.weight\", \"bert.bert.encoder.layer.7.output.dense.bias\", \"bert.bert.encoder.layer.7.output.LayerNorm.weight\", \"bert.bert.encoder.layer.7.output.LayerNorm.bias\", \"bert.bert.encoder.layer.8.attention.self.query.weight\", \"bert.bert.encoder.layer.8.attention.self.query.bias\", \"bert.bert.encoder.layer.8.attention.self.key.weight\", \"bert.bert.encoder.layer.8.attention.self.key.bias\", \"bert.bert.encoder.layer.8.attention.self.value.weight\", \"bert.bert.encoder.layer.8.attention.self.value.bias\", \"bert.bert.encoder.layer.8.attention.output.dense.weight\", \"bert.bert.encoder.layer.8.attention.output.dense.bias\", \"bert.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.8.intermediate.dense.weight\", \"bert.bert.encoder.layer.8.intermediate.dense.bias\", \"bert.bert.encoder.layer.8.output.dense.weight\", \"bert.bert.encoder.layer.8.output.dense.bias\", \"bert.bert.encoder.layer.8.output.LayerNorm.weight\", \"bert.bert.encoder.layer.8.output.LayerNorm.bias\", \"bert.bert.encoder.layer.9.attention.self.query.weight\", \"bert.bert.encoder.layer.9.attention.self.query.bias\", \"bert.bert.encoder.layer.9.attention.self.key.weight\", \"bert.bert.encoder.layer.9.attention.self.key.bias\", \"bert.bert.encoder.layer.9.attention.self.value.weight\", \"bert.bert.encoder.layer.9.attention.self.value.bias\", \"bert.bert.encoder.layer.9.attention.output.dense.weight\", \"bert.bert.encoder.layer.9.attention.output.dense.bias\", \"bert.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.9.intermediate.dense.weight\", \"bert.bert.encoder.layer.9.intermediate.dense.bias\", \"bert.bert.encoder.layer.9.output.dense.weight\", \"bert.bert.encoder.layer.9.output.dense.bias\", \"bert.bert.encoder.layer.9.output.LayerNorm.weight\", \"bert.bert.encoder.layer.9.output.LayerNorm.bias\", \"bert.bert.encoder.layer.10.attention.self.query.weight\", \"bert.bert.encoder.layer.10.attention.self.query.bias\", \"bert.bert.encoder.layer.10.attention.self.key.weight\", \"bert.bert.encoder.layer.10.attention.self.key.bias\", \"bert.bert.encoder.layer.10.attention.self.value.weight\", \"bert.bert.encoder.layer.10.attention.self.value.bias\", \"bert.bert.encoder.layer.10.attention.output.dense.weight\", \"bert.bert.encoder.layer.10.attention.output.dense.bias\", \"bert.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.10.intermediate.dense.weight\", \"bert.bert.encoder.layer.10.intermediate.dense.bias\", \"bert.bert.encoder.layer.10.output.dense.weight\", \"bert.bert.encoder.layer.10.output.dense.bias\", \"bert.bert.encoder.layer.10.output.LayerNorm.weight\", \"bert.bert.encoder.layer.10.output.LayerNorm.bias\", \"bert.bert.encoder.layer.11.attention.self.query.weight\", \"bert.bert.encoder.layer.11.attention.self.query.bias\", \"bert.bert.encoder.layer.11.attention.self.key.weight\", \"bert.bert.encoder.layer.11.attention.self.key.bias\", \"bert.bert.encoder.layer.11.attention.self.value.weight\", \"bert.bert.encoder.layer.11.attention.self.value.bias\", \"bert.bert.encoder.layer.11.attention.output.dense.weight\", \"bert.bert.encoder.layer.11.attention.output.dense.bias\", \"bert.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"bert.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"bert.bert.encoder.layer.11.intermediate.dense.weight\", \"bert.bert.encoder.layer.11.intermediate.dense.bias\", \"bert.bert.encoder.layer.11.output.dense.weight\", \"bert.bert.encoder.layer.11.output.dense.bias\", \"bert.bert.encoder.layer.11.output.LayerNorm.weight\", \"bert.bert.encoder.layer.11.output.LayerNorm.bias\", \"bert.bert.pooler.dense.weight\", \"bert.bert.pooler.dense.bias\". "
     ]
    }
   ],
   "source": [
    "# On désérialise le meilleur modèle enregistré :\n",
    "model = EntityModel(num_tag=num_tag)\n",
    "model.load_state_dict(torch.load('data/models/model_trained_en.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05621773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:39:58.538011Z",
     "start_time": "2022-06-26T08:39:58.446365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minister said the man and a family member were unharmed because they got out of the vehicle just before the blast .\n"
     ]
    }
   ],
   "source": [
    "i = 423\n",
    "single_example = test_dataset[i]\n",
    "print(' '.join(test_dataset.texts[i]))\n",
    "\n",
    "single_example = {k: single_example[k].unsqueeze(0) for k in single_example}\n",
    "with torch.no_grad():\n",
    "    output, loss = model(**single_example)\n",
    "\n",
    "# Retrait des valeurs qui ne contribuent pas à la loss :\n",
    "output = output[single_example['mask'] == 1]\n",
    "\n",
    "predictions_probas = nn.functional.softmax(output, dim=1).detach().squeeze()\n",
    "predictions_probas, predictions_classes = torch.max(predictions_probas, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85364d9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:39:58.738648Z",
     "start_time": "2022-06-26T08:39:58.689779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token_text</th>\n",
       "      <td>[CLS]</td>\n",
       "      <td>the</td>\n",
       "      <td>minister</td>\n",
       "      <td>said</td>\n",
       "      <td>the</td>\n",
       "      <td>man</td>\n",
       "      <td>and</td>\n",
       "      <td>a</td>\n",
       "      <td>family</td>\n",
       "      <td>member</td>\n",
       "      <td>were</td>\n",
       "      <td>unharmed</td>\n",
       "      <td>because</td>\n",
       "      <td>they</td>\n",
       "      <td>got</td>\n",
       "      <td>out</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>vehicle</td>\n",
       "      <td>just</td>\n",
       "      <td>before</td>\n",
       "      <td>the</td>\n",
       "      <td>blast</td>\n",
       "      <td>.</td>\n",
       "      <td>[SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_id</th>\n",
       "      <td>101</td>\n",
       "      <td>1996</td>\n",
       "      <td>2704</td>\n",
       "      <td>2056</td>\n",
       "      <td>1996</td>\n",
       "      <td>2158</td>\n",
       "      <td>1998</td>\n",
       "      <td>1037</td>\n",
       "      <td>2155</td>\n",
       "      <td>2266</td>\n",
       "      <td>2020</td>\n",
       "      <td>28150</td>\n",
       "      <td>2138</td>\n",
       "      <td>2027</td>\n",
       "      <td>2288</td>\n",
       "      <td>2041</td>\n",
       "      <td>1997</td>\n",
       "      <td>1996</td>\n",
       "      <td>4316</td>\n",
       "      <td>2074</td>\n",
       "      <td>2077</td>\n",
       "      <td>1996</td>\n",
       "      <td>8479</td>\n",
       "      <td>1012</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_code</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred_label</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_true</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_proba</th>\n",
       "      <td>0.999074</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999329</td>\n",
       "      <td>0.499869</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.998384</td>\n",
       "      <td>0.998224</td>\n",
       "      <td>0.99505</td>\n",
       "      <td>0.999293</td>\n",
       "      <td>0.998228</td>\n",
       "      <td>0.999462</td>\n",
       "      <td>0.998263</td>\n",
       "      <td>0.998102</td>\n",
       "      <td>0.999383</td>\n",
       "      <td>0.99857</td>\n",
       "      <td>0.998542</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.998332</td>\n",
       "      <td>0.998324</td>\n",
       "      <td>0.499777</td>\n",
       "      <td>0.999562</td>\n",
       "      <td>0.999582</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0         1         2         3         4         5   \\\n",
       "token_text      [CLS]       the  minister      said       the       man   \n",
       "token_id          101      1996      2704      2056      1996      2158   \n",
       "pred_code           0         0         0         0         0         0   \n",
       "pred_label          O         O         O         O         O         O   \n",
       "y_true              O         O         O         O         O         O   \n",
       "class_proba  0.999074  0.999998  0.999329  0.499869  0.999999  0.333333   \n",
       "\n",
       "                   6         7        8         9         10        11  \\\n",
       "token_text        and         a   family    member      were  unharmed   \n",
       "token_id         1998      1037     2155      2266      2020     28150   \n",
       "pred_code           0         0        0         0         0         0   \n",
       "pred_label          O         O        O         O         O         O   \n",
       "y_true              O         O        O         O         O         O   \n",
       "class_proba  0.998384  0.998224  0.99505  0.999293  0.998228  0.999462   \n",
       "\n",
       "                   12        13        14       15        16        17  \\\n",
       "token_text    because      they       got      out        of       the   \n",
       "token_id         2138      2027      2288     2041      1997      1996   \n",
       "pred_code           0         0         0        0         0         0   \n",
       "pred_label          O         O         O        O         O         O   \n",
       "y_true              O         O         O        O         O         O   \n",
       "class_proba  0.998263  0.998102  0.999383  0.99857  0.998542  0.999997   \n",
       "\n",
       "                   18        19        20        21        22        23  \\\n",
       "token_text    vehicle      just    before       the     blast         .   \n",
       "token_id         4316      2074      2077      1996      8479      1012   \n",
       "pred_code           0         0         0         0         0         0   \n",
       "pred_label          O         O         O         O         O         O   \n",
       "y_true              O         O         O         O         O         O   \n",
       "class_proba  0.998332  0.998324  0.499777  0.999562  0.999582  0.999999   \n",
       "\n",
       "                   24  \n",
       "token_text      [SEP]  \n",
       "token_id          102  \n",
       "pred_code           0  \n",
       "pred_label          O  \n",
       "y_true              O  \n",
       "class_proba  0.999997  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_tokens = single_example['mask'].sum().item()\n",
    "ids = single_example['ids'].squeeze()[:nb_tokens]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'token_text': [EntityDataset.tokenizer.decode([token]) for token in ids],\n",
    "    'token_id': ids,\n",
    "    'pred_code': predictions_classes,\n",
    "    'pred_label': ordinal_enc_NER.inverse_transform(predictions_classes.reshape(-1, 1)).squeeze(),\n",
    "    'y_true': ordinal_enc_NER.inverse_transform(single_example['target_tag'].squeeze()[:nb_tokens].reshape(-1, 1)).squeeze(),\n",
    "    'class_proba': predictions_probas\n",
    "}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "08f8a287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:39:18.915473Z",
     "start_time": "2022-06-26T08:39:18.891398Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Official',\n",
       " 'preliminary',\n",
       " 'results',\n",
       " 'are',\n",
       " 'expected',\n",
       " 'to',\n",
       " 'be',\n",
       " 'released',\n",
       " 'later',\n",
       " 'Thursday',\n",
       " '.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1eb5bba7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-26T08:39:19.994812Z",
     "start_time": "2022-06-26T08:39:19.969453Z"
    },
    "cell_style": "split"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_test[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1e168b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-06T16:25:07.435286Z",
     "start_time": "2022-06-06T16:25:07.407011Z"
    }
   },
   "source": [
    "On remarque que la base d'entraînement est parfois mal labélisée... cf exemples 415, ou 4222. Sur l'exemple `sentences_test[4222]`, on voit que le modèle arrive même à corriger une erreur de labélisation dans l'échantillon test !"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.11.1"
   }
  },
  "kernelspec": {
   "display_name": "parc_v1",
   "language": "python",
   "name": "parc_v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
